{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "x_train = pd.read_csv('../data/fill_data/x_train.csv')\n",
    "x_valid = pd.read_csv('../data/fill_data/x_valid.csv')\n",
    "y_train = pd.read_csv('../data/fill_data/y_train.csv')\n",
    "y_valid = pd.read_csv('../data/fill_data/y_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_valid = x_valid.values\n",
    "y_train = y_train.values\n",
    "y_valid = y_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).ravel()\n",
    "y_valid = np.array(y_valid).ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 13 and best_valid_accuracy = 0.82365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간 : 775.2704820632935\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf = TabNetClassifier(\n",
    "    n_d=56, \n",
    "    n_steps=3, gamma=1.4,\n",
    "    lambda_sparse=1.1694994589239397e-05, \n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "    mask_type='sparsemax',\n",
    "    n_shared=2,\n",
    "    scheduler_params=dict(mode=\"min\",\n",
    "        patience = 3, # changing sheduler patience to be lower than early stopping patience\n",
    "        min_lr=1e-5,factor=0.5,),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    verbose=0)\n",
    "clf.fit(x_train, y_train,\n",
    "    eval_set=[(x_train, y_train),(x_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['accuracy'],\n",
    "    drop_last = False,\n",
    "    patience = 29, max_epochs = 80\n",
    ")\n",
    "end_time = time.time()\n",
    "print('걸린 시간 :', end_time - start_time)\n",
    "\n",
    "## 기본값: 0.81477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8240567988140021\n",
      "f1_score:  0.9012676853423883\n",
      "Accuracy:  0.8236545682102628\n",
      "f1_score:  0.9008514530997115\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "# train data\n",
    "train_pred_y = clf.predict(x_train)\n",
    "print('Accuracy: ', (accuracy_score(y_train, train_pred_y)))\n",
    "print('f1_score: ', f1_score(y_train, train_pred_y))\n",
    "\n",
    "# valid data\n",
    "valid_pred_y = clf.predict(x_valid)\n",
    "print('Accuracy: ', (accuracy_score(y_valid, valid_pred_y)))\n",
    "print('f1_score: ', f1_score(y_valid, valid_pred_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(trial : Trial, x_train, y_train, x_valid, y_valid):\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n",
    "    n_da = trial.suggest_int(\"n_da\", 56, 64, step=4)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 1, 3, step=1)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1., 1.4, step=0.2)\n",
    "    n_shared = trial.suggest_int(\"n_shared\", 1, 3)\n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "        mask_type=mask_type, n_shared=n_shared,\n",
    "        scheduler_params=dict(mode=\"min\",\n",
    "        patience=trial.suggest_int(\"patienceScheduler\",low=3,high=10), # changing sheduler patience to be lower than early stopping patience\n",
    "        min_lr=1e-5,factor=0.5,),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose=0,\n",
    "        ) #early stopping\n",
    "    \n",
    "    clf = TabNetClassifier(**tabnet_params)\n",
    "    clf.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "        patience=trial.suggest_int(\"patience\",low=15,high=30), max_epochs=trial.suggest_int('epochs', 1, 100),\n",
    "        eval_metric=['accuracy'])\n",
    "\n",
    "    score = f1_score(clf.predict(x_valid), y_valid)\n",
    "    \n",
    "    return score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-31 22:39:37,221]\u001B[0m A new study created in memory with name: TabNet optimization\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 17 and best_val_1_accuracy = 0.81615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-01-31 23:02:16,646]\u001B[0m Trial 0 finished with value: 0.8973516875131019 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 3, 'gamma': 1.0, 'n_shared': 3, 'lambda_sparse': 0.0008333446335986498, 'patienceScheduler': 5, 'patience': 18, 'epochs': 92}. Best is trial 0 with value: 0.8973516875131019.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 5 and best_val_1_accuracy = 0.82591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-01-31 23:15:30,668]\u001B[0m Trial 1 finished with value: 0.9021731486039806 and parameters: {'mask_type': 'sparsemax', 'n_da': 60, 'n_steps': 2, 'gamma': 1.2, 'n_shared': 2, 'lambda_sparse': 3.848386554022774e-05, 'patienceScheduler': 5, 'patience': 30, 'epochs': 43}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 3 and best_val_1_accuracy = 0.82178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-01-31 23:22:37,709]\u001B[0m Trial 2 finished with value: 0.8995059985885674 and parameters: {'mask_type': 'entmax', 'n_da': 56, 'n_steps': 1, 'gamma': 1.4, 'n_shared': 3, 'lambda_sparse': 0.00041663519224314756, 'patienceScheduler': 6, 'patience': 21, 'epochs': 71}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 61 with best_epoch = 40 and best_val_1_accuracy = 0.81352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-01-31 23:51:59,471]\u001B[0m Trial 3 finished with value: 0.8959642508029606 and parameters: {'mask_type': 'entmax', 'n_da': 56, 'n_steps': 2, 'gamma': 1.0, 'n_shared': 3, 'lambda_sparse': 0.0001264267226576953, 'patienceScheduler': 7, 'patience': 30, 'epochs': 61}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 5 and best_val_1_accuracy = 0.82416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:05:32,904]\u001B[0m Trial 4 finished with value: 0.9009237712432128 and parameters: {'mask_type': 'sparsemax', 'n_da': 60, 'n_steps': 3, 'gamma': 1.4, 'n_shared': 1, 'lambda_sparse': 0.0001019834075272744, 'patienceScheduler': 9, 'patience': 28, 'epochs': 58}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 11 with best_epoch = 10 and best_val_1_accuracy = 0.81539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:08:38,890]\u001B[0m Trial 5 finished with value: 0.8969468315517362 and parameters: {'mask_type': 'entmax', 'n_da': 56, 'n_steps': 2, 'gamma': 1.0, 'n_shared': 1, 'lambda_sparse': 8.490916649868237e-06, 'patienceScheduler': 3, 'patience': 27, 'epochs': 11}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 5 and best_val_1_accuracy = 0.81865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:21:26,477]\u001B[0m Trial 6 finished with value: 0.8986075152193689 and parameters: {'mask_type': 'entmax', 'n_da': 56, 'n_steps': 2, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 6.249938756793775e-06, 'patienceScheduler': 10, 'patience': 30, 'epochs': 36}. Best is trial 1 with value: 0.9021731486039806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 13 with best_epoch = 5 and best_val_1_accuracy = 0.82691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:28:30,036]\u001B[0m Trial 7 finished with value: 0.9024751427966998 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 2, 'gamma': 1.4, 'n_shared': 3, 'lambda_sparse': 9.03897160309517e-05, 'patienceScheduler': 9, 'patience': 15, 'epochs': 13}. Best is trial 7 with value: 0.9024751427966998.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 3 and best_val_1_accuracy = 0.82178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:35:04,554]\u001B[0m Trial 8 finished with value: 0.9000561482313307 and parameters: {'mask_type': 'entmax', 'n_da': 64, 'n_steps': 2, 'gamma': 1.2, 'n_shared': 1, 'lambda_sparse': 5.5943576551246625e-06, 'patienceScheduler': 8, 'patience': 17, 'epochs': 37}. Best is trial 7 with value: 0.9024751427966998.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 3 and best_val_1_accuracy = 0.82203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/bitamin/ChurnProject/venv/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001B[32m[I 2023-02-01 00:42:41,579]\u001B[0m Trial 9 finished with value: 0.8999577880962432 and parameters: {'mask_type': 'entmax', 'n_da': 60, 'n_steps': 1, 'gamma': 1.0, 'n_shared': 3, 'lambda_sparse': 3.6794820151366092e-06, 'patienceScheduler': 9, 'patience': 19, 'epochs': 93}. Best is trial 7 with value: 0.9024751427966998.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial : score 0.9024751427966998, \n",
      "params {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 2, 'gamma': 1.4, 'n_shared': 3, 'lambda_sparse': 9.03897160309517e-05, 'patienceScheduler': 9, 'patience': 15, 'epochs': 13}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler = TPESampler(), study_name='TabNet optimization')\n",
    "study.optimize(lambda trial : Objective(trial, x_train, y_train, x_valid, y_valid), n_trials = 10)\n",
    "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10bf58d2cb1d6fe77a6d789fab1958d6f68caa67042b20c480a5cc6dc8179d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
